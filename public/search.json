[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hushh Labs\nHushh Labs is a research and development lab focused on exploring the potential in the intersection of AI and privacy. We are a team of engineers, scientists, and entrepreneurs who are passionate about creating a future where privacy is respected and AI is used to empower individuals.\nOur mission is to create products and services that enable individuals to take control of their data and use it to their advantage. We believe that privacy is a fundamental right and that AI can be used to protect it.\nWe are currently working on a number of projects that explore the potential of AI and privacy. Our goal is to create products and services that make it easier for individuals to protect their data and use it to their advantage.\nWe are always looking for talented individuals to join our team. If you are interested in learning more about our work or joining our team, please reach out to us. We look forward to hearing from you!\ninfo@hush1one.com\n\n\n\nhushh labs"
  },
  {
    "objectID": "posts/image_search_with_text.html",
    "href": "posts/image_search_with_text.html",
    "title": "Search and Discovery for Luxury Fashion",
    "section": "",
    "text": "Enhancing the search for the expression of self"
  },
  {
    "objectID": "posts/image_search_with_text.html#unleashing-the-potential-of-llm-embeddings",
    "href": "posts/image_search_with_text.html#unleashing-the-potential-of-llm-embeddings",
    "title": "Search and Discovery for Luxury Fashion",
    "section": "Unleashing the Potential of LLM Embeddings",
    "text": "Unleashing the Potential of LLM Embeddings\nLarge Language Models, such as GPT-3.5, possess remarkable natural language processing capabilities. Trained on massive amounts of textual data, these models acquire a deep understanding of language nuances, syntax, and semantics. LLM embeddings are numerical representations derived from text, capturing the intricate contextual information inherent in sentences, paragraphs, or documents."
  },
  {
    "objectID": "posts/image_search_with_text.html#why-embedding-search-for-stylistic-search",
    "href": "posts/image_search_with_text.html#why-embedding-search-for-stylistic-search",
    "title": "Search and Discovery for Luxury Fashion",
    "section": "Why Embedding Search for Stylistic Search?",
    "text": "Why Embedding Search for Stylistic Search?\nBy harnessing the power of LLM embeddings, we unlock a number of advantages for searching or navigating a particular style:\n\nEnhanced Semantic Relevance LLM embeddings encapsulate a profound comprehension of language semantics, leading to remarkably accurate search results. In the context of luxury fashion, where precision and exquisite details matter, LLM embeddings grasp the subtle nuances and connotations associated with high-end fashion. This semantic awareness ensures that search results align with the desired level of opulence, refinement, and aesthetic appeal.\nFashion-Specific Vocabulary and Concepts Luxury fashion is an industry replete with distinct terminologies and industry-specific jargon. Traditional TF/IDF search may struggle to capture the essence of these nuanced fashion terms, leading to suboptimal search outcomes. Conversely, LLM embeddings excel in comprehending fashion-specific vocabulary, recognizing brand names, fashion trends, and intricate design elements. This prowess enables users to delve into the world of luxury fashion with unparalleled accuracy.\nContext-Aware Search In the realm of luxury fashion, context plays a pivotal role in understanding the true essence of a product. LLM embeddings excel at capturing the contextual information present in textual descriptions or queries, leading to more refined and contextually appropriate image search results. Users can now effortlessly explore images that encapsulate the desired style, occasion, or fashion statement, ensuring a seamless and personalized luxury fashion journey.\nUniting Text and Visuals The marriage of text and visuals is at the heart of luxury fashion image search. LLM embeddings act as a bridge, facilitating a seamless connection between the textual descriptions and the visual representation of fashion products. By leveraging LLM embeddings, users can effortlessly navigate the realm of luxury fashion, exploring visually captivating images that align with their refined tastes and preferences.\n\nIn conclusion, the use of LLM embeddings for luxury fashion image search transcends the limitations of conventional TF/IDF search. By harnessing the semantic understanding, fashion-specific vocabulary, and contextual awareness ingrained in LLM embeddings, users can embark on a visually enchanting journey through the world of luxury fashion. Experience the epitome of elegance and style like never before, with LLM-powered image search in luxury fashion."
  },
  {
    "objectID": "posts/survey_on_llm_privacy.html",
    "href": "posts/survey_on_llm_privacy.html",
    "title": "A Survey on Language Models and Related Data Privacy",
    "section": "",
    "text": "AI Privacy"
  },
  {
    "objectID": "posts/survey_on_llm_privacy.html#abstract",
    "href": "posts/survey_on_llm_privacy.html#abstract",
    "title": "A Survey on Language Models and Related Data Privacy",
    "section": "Abstract",
    "text": "Abstract\nThis research survey paper delves into the current revolution of Language models, specifically Large Language models (LLMs) and Fine-tuned models(FTMs). It explores the accessibility of these models across various domains of work while emphasizing the importance of privacy concerns when interacting with on-cloud LLMs. The study examines the influence of pre-training data, training data, and test data on the performance and capabilities of language models. Furthermore, it provides a comprehensive analysis of the potential use cases and limitations of large language models in different natural language processing tasks. These tasks include knowledgeintensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and specific task considerations. Given that training models often require extensive and representative datasets, which may contain sensitive information, it becomes crucial to protect user privacy. The paper discusses algorithmic techniques for learning and conducts refined analysis of privacy costs within the framework of differential privacy. It explores interrelated concepts associated with differential privacy, such as privacy loss, mechanisms of differential privacy, local and centralized differential privacy, and the applications of differential privacy in statistics, machine learning, and federated learning. By addressing the aforementioned aspects, this survey paper contributes to the understanding of language models’ revolution, their accessibility across domains, privacy concerns, and the incorporation of differential privacy to mitigate privacy risks."
  },
  {
    "objectID": "posts/survey_on_llm_privacy.html#discussions-on-language-models",
    "href": "posts/survey_on_llm_privacy.html#discussions-on-language-models",
    "title": "A Survey on Language Models and Related Data Privacy",
    "section": "Discussions on Language Models",
    "text": "Discussions on Language Models\nLets start from the very basics, Large Language model, which in recent times have been in treads have its roots in Natural Language processing.The subject of research known as “Natural Language Processing” (NLP) focuses on how computers can comprehend and interact with human language. It entails training computers to comprehend, decipher, and produce human language in a manner akin to how we humans speak. Language models like GPT are a significant NLP application. By analyzing a sizable amount of training data, LLMs are created to comprehend and produce writing that resembles that of humans. They develop an understanding of the structure, syntax, and meaning of words and phrases, which enables them to produce coherent and appropriately situated replies.\\ To understand the abilities of large language models (LLMs), we compare them with fine-tuned models. LLMs are really big language models that are trained on lots of data without being specifically adjusted for particular tasks. On the other hand, fine-tuned models are usually smaller language models that are also trained and then further adjusted on a smaller dataset that focuses on a specific task. In simple terms, fine-tuned models are more specialized and optimized for specific tasks compared to LLMs.\nNow, let’s talk about some practical applications of language models. One important application is natural language understanding. LLMs are really good at understanding and making sense of human language, even when they encounter new or unfamiliar data. This makes them useful for tasks that involve understanding language in different contexts or with very little training data.\nAnother application is natural language generation. LLMs have the ability to generate text that is coherent, relevant, and of high quality. This can be utilized in various applications where we need computers to create text, such as writing articles, generating chatbot responses, or even creating stories.\nLanguage models are also helpful in knowledge-intensive tasks. LLMs have been trained on vast amounts of data, so they store a lot of knowledge about different domains or general information about the world. This knowledge can be used to assist in tasks that require specific expertise or general understanding.\nLastly, language models can improve reasoning abilities. LLMs are designed to understand patterns and relationships in language, which can be useful for decision-making and problem-solving in different situations. By utilizing the reasoning capabilities of LLMs, we can enhance our ability to make better decisions and solve complex problems.\nIn the realm of large language models (LLMs), researchers employ different training strategies, model architectures, and use cases. To better understand them, these models can be classified into two main categories: encoder-only language models and decoder-only language models.\nEncoder-only language models, also known as Encoder-Decoder models or BERT-style language models, are utilized when there is readily available natural language data. These models are trained using a technique called the Masked Language Model, where the model predicts masked words in a sentence while considering the surrounding context. This training approach allows the model to develop a deeper understanding of the relationships between words and the context in which they are used. These models typically employ the Transformer architecture, a powerful deep learning model that excels at processing and comprehending natural language.\nOn the other hand, decoder-only language models, such as GPT-style language models, are designed to understand and generate human-like text. These models analyze patterns from a large amount of training data and predict what comes next in a given sequence of words. Unlike encoder-only models, decoder-only models focus on generating text rather than understanding it in a conversational context. They can be employed for tasks like generating creative writing, answering questions, or assisting with language-related tasks. These models are trained as Autoregressive Language Models, where they generate the next word in a sequence based on the preceding words. This type of training, combined with the ability to learn from prompts and in-context information, demonstrates the superiority of autoregressive language models.\nApdditionally, both encoder-only and decoder-only models benefit from few-shot and zero-shot learning. Few-shot learning enables the models to learn new things with just a few examples, while zero-shot learning allows them to learn completely new concepts without any examples at all. These approaches enable the models to perform well on tasks they haven’t been explicitly trained for, leveraging prior knowledge and transferring knowledge from related tasks."
  },
  {
    "objectID": "posts/manifesto.html",
    "href": "posts/manifesto.html",
    "title": "Hushh Manifesto",
    "section": "",
    "text": "# Hushh - Redefining Access, Control, Intelligence, and Monetization\nAt Hushh, we believe in building a startup that empowers individuals by redefining the principles of Access, Control, Intelligence, and Monetization. Our platform aims to revolutionize the way people interact, share, and derive value from their data, ensuring their privacy, choice, and ability to create value. This manifesto outlines the core principles that drive our mission and guide our actions."
  },
  {
    "objectID": "posts/manifesto.html#access",
    "href": "posts/manifesto.html#access",
    "title": "Hushh Manifesto",
    "section": "Access:",
    "text": "Access:\nWe believe that access to information and resources should be inclusive, equitable, and easily attainable for everyone. Hushh aims to break down barriers and provide equal opportunities to all individuals, regardless of their background, location, or socio-economic status. Through our platform, we strive to create an ecosystem that fosters open communication, collaboration, and shared knowledge."
  },
  {
    "objectID": "posts/manifesto.html#control-choice",
    "href": "posts/manifesto.html#control-choice",
    "title": "Hushh Manifesto",
    "section": "Control (Choice):",
    "text": "Control (Choice):\nWe recognize that individuals should have complete control over their personal data and online presence. At Hushh, we prioritize privacy and provide users with the tools to make informed decisions about the information they share and how it is used. We respect the autonomy and agency of our users, empowering them to define their own digital experiences and safeguard their personal information."
  },
  {
    "objectID": "posts/manifesto.html#intelligence",
    "href": "posts/manifesto.html#intelligence",
    "title": "Hushh Manifesto",
    "section": "Intelligence :",
    "text": "Intelligence :\nWe believe that intelligence is the foundation of progress and growth. At Hushh, we leverage cutting-edge technologies, such as artificial intelligence and machine learning, to provide intelligent insights and personalized experiences. Our platform understands user preferences, adapts to their needs, and continuously learns to enhance the overall user experience. We harness the power of intelligence to empower individuals and help them make better decisions in their personal and professional lives."
  },
  {
    "objectID": "posts/manifesto.html#monetization-value-creation-for-the-user-of-the-platform",
    "href": "posts/manifesto.html#monetization-value-creation-for-the-user-of-the-platform",
    "title": "Hushh Manifesto",
    "section": "Monetization (Value Creation for the User of the Platform):",
    "text": "Monetization (Value Creation for the User of the Platform):\nWe understand the importance of value creation for our users. Hushh is committed to creating a sustainable ecosystem that enables individuals to monetize their skills, knowledge, and creations. We provide tools and opportunities for users to showcase their expertise, connect with like-minded individuals, and explore avenues for financial growth. Through our platform, users can unlock their full potential and realize the value they bring to the digital world.\nTogether, we envision a future where access is universal, control is in the hands of individuals, intelligence is seamlessly integrated, and monetization is fair and rewarding. At Hushh, we are dedicated to building a community-driven platform that challenges existing norms, fosters innovation, and empowers individuals to thrive in the digital age. Join us on this transformative journey as we shape a world where everyone’s voice is heard, respected, and valued. Together, we can build a future that embraces the true potential of technology and human ingenuity."
  },
  {
    "objectID": "posts/logo_analysis.html",
    "href": "posts/logo_analysis.html",
    "title": "Logo Analysis for Brand Affinity",
    "section": "",
    "text": "Logo Analysis\n\n\nIn today’s digital age, retailers are constantly seeking innovative ways to better understand their customers and build brand affinity. One powerful tool that is revolutionizing the retail industry is Hushh, a customer wallet platform that utilizes machine learning to analyze personal images and identify brand logos. By leveraging the power of computer vision and deep learning algorithms, Hushh provides retailers with valuable insights into customer preferences and behavior.\n\n\nHushh’s computer vision platform uses a combination of deep learning and computer vision techniques to identify logos in personal images. Whether it’s a logo on clothing, accessories, or products, Hushh can detect and analyze it. Once the logo is identified, the platform then determines the brand associated with it.\n\n\n\nOne of the key features of Hushh is its ability to track customer preferences over time. By analyzing personal images and customer sentiment, the platform provides retailers with valuable insights into how customer preferences evolve. This information is crucial for retailers to tailor their marketing strategies and offerings accordingly.\nBrands are a powerful way for users to express their personal preferences publicly. By capturing brand preference over time, retailers can gain a deeper and more immediate insight into what a customer is interested in when they arrive at the store.\nHushh is working on novel information displays to express customer brand preference. The following widget shows a force directed bubble plot of a customer’s favorite brands, sized according to how often that brand appears in their personal images. The widget below is fully interactive, enabling intuitive browsing of brands and brand categories.\n\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Hushh Labs&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Hushh Labs&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.xml\"&gt;/index.xml&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Hushh Labs - Logo Analysis for Brand Affinity&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Hushh Labs - Logo Analysis for Brand Affinity&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Hushh Labs - Logo Analysis for Brand Affinity&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Hushh Labs&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;Customers regularly express preferences for brands based on their attire. How can retail adapt and learn without compromising privacy?&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;Customers regularly express preferences for brands based on their attire. How can retail adapt and learn without compromising privacy?&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/logo_analysis.html#how-does-hushh-logo-analysis-work",
    "href": "posts/logo_analysis.html#how-does-hushh-logo-analysis-work",
    "title": "Logo Analysis for Brand Affinity",
    "section": "",
    "text": "Hushh’s computer vision platform uses a combination of deep learning and computer vision techniques to identify logos in personal images. Whether it’s a logo on clothing, accessories, or products, Hushh can detect and analyze it. Once the logo is identified, the platform then determines the brand associated with it."
  },
  {
    "objectID": "posts/logo_analysis.html#capturing-brand-preference-over-time",
    "href": "posts/logo_analysis.html#capturing-brand-preference-over-time",
    "title": "Logo Analysis for Brand Affinity",
    "section": "",
    "text": "One of the key features of Hushh is its ability to track customer preferences over time. By analyzing personal images and customer sentiment, the platform provides retailers with valuable insights into how customer preferences evolve. This information is crucial for retailers to tailor their marketing strategies and offerings accordingly.\nBrands are a powerful way for users to express their personal preferences publicly. By capturing brand preference over time, retailers can gain a deeper and more immediate insight into what a customer is interested in when they arrive at the store.\nHushh is working on novel information displays to express customer brand preference. The following widget shows a force directed bubble plot of a customer’s favorite brands, sized according to how often that brand appears in their personal images. The widget below is fully interactive, enabling intuitive browsing of brands and brand categories.\n\n&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Hushh Labs&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Hushh Labs&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:Home\"&gt;Home&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.html\"&gt;/index.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:About\"&gt;About&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/about.html\"&gt;/about.html&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar:/index.xml\"&gt;/index.xml&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Hushh Labs - Logo Analysis for Brand Affinity&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Hushh Labs - Logo Analysis for Brand Affinity&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Hushh Labs - Logo Analysis for Brand Affinity&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Hushh Labs&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;Customers regularly express preferences for brands based on their attire. How can retail adapt and learn without compromising privacy?&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;Customers regularly express preferences for brands based on their attire. How can retail adapt and learn without compromising privacy?&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hushh Labs",
    "section": "",
    "text": "Hushh Manifesto\n\n\n\n\n\n\n\nmission\n\n\nprinciples\n\n\n\n\nWhat does Hushh believe? What are we working towards?\n\n\n\n\n\n\nJul 11, 2023\n\n\nManish Sainani, Justin Donaldson, Sunaz Sharaf\n\n\n\n\n\n\n  \n\n\n\n\nA Survey on Language Models and Related Data Privacy\n\n\n\n\n\n\n\nLLM\n\n\nPrivacy\n\n\n\n\nCustomers regularly express preferences for brands based on their attire. How can retail adapt and learn without compromising privacy?\n\n\n\n\n\n\nJul 11, 2023\n\n\nHarshvardhan, Justin Donaldson, Manish Sainani\n\n\n\n\n\n\n  \n\n\n\n\nLogo Analysis for Brand Affinity\n\n\n\n\n\n\n\nfashion\n\n\nML\n\n\nvision\n\n\n\n\nCustomers regularly express preferences for brands based on their attire. How can retail adapt and learn without compromising privacy?\n\n\n\n\n\n\nJul 11, 2023\n\n\nJustin Donaldson\n\n\n\n\n\n\n  \n\n\n\n\nSearch and Discovery for Luxury Fashion\n\n\n\n\n\n\n\nsearch\n\n\nembedding\n\n\ndiscovery\n\n\n\n\nSearch and discovery go hand in hand when finding the frontiers for personal taste\n\n\n\n\n\n\nJul 10, 2023\n\n\nJustin Donaldson\n\n\n\n\n\n\nNo matching items"
  }
]