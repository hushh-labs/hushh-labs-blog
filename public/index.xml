<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Hushh Labs</title>
<link>https://hushh_labs.gitlab.io/gitlab-profile/index.html</link>
<atom:link href="https://hushh_labs.gitlab.io/gitlab-profile/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.3.433</generator>
<lastBuildDate>Tue, 11 Jul 2023 07:00:00 GMT</lastBuildDate>
<item>
  <title>A Survey on Language Models and Related Data Privacy</title>
  <dc:creator>Harshvardhan, Justin Donaldson, Manish Sainani</dc:creator>
  <link>https://hushh_labs.gitlab.io/gitlab-profile/posts/survey_on_llm_privacy.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hushh_labs.gitlab.io/gitlab-profile/posts/images/omgjjd_data_privacy_AI_home_secure_f87b681d-8207-46b6-adf4-4e2981b09f18.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">AI Privacy</figcaption>
</figure>
</div>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>This research survey paper delves into the current revolution of Language models, specifically Large Language models (LLMs) and Fine-tuned models(FTMs). It explores the accessibility of these models across various domains of work while emphasizing the importance of privacy concerns when interacting with on-cloud LLMs. The study examines the influence of pre-training data, training data, and test data on the performance and capabilities of language models. Furthermore, it provides a comprehensive analysis of the potential use cases and limitations of large language models in different natural language processing tasks. These tasks include knowledgeintensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and specific task considerations. Given that training models often require extensive and representative datasets, which may contain sensitive information, it becomes crucial to protect user privacy. The paper discusses algorithmic techniques for learning and conducts refined analysis of privacy costs within the framework of differential privacy. It explores interrelated concepts associated with differential privacy, such as privacy loss, mechanisms of differential privacy, local and centralized differential privacy, and the applications of differential privacy in statistics, machine learning, and federated learning. By addressing the aforementioned aspects, this survey paper contributes to the understanding of language models’ revolution, their accessibility across domains, privacy concerns, and the incorporation of differential privacy to mitigate privacy risks.</p>
</section>
<section id="discussions-on-language-models" class="level2">
<h2 class="anchored" data-anchor-id="discussions-on-language-models">Discussions on Language Models</h2>
<p>Lets start from the very basics, Large Language model, which in recent times have been in treads have its roots in Natural Language processing.The subject of research known as “Natural Language Processing” (NLP) focuses on how computers can comprehend and interact with human language. It entails training computers to comprehend, decipher, and produce human language in a manner akin to how we humans speak. Language models like GPT are a significant NLP application. By analyzing a sizable amount of training data, LLMs are created to comprehend and produce writing that resembles that of humans. They develop an understanding of the structure, syntax, and meaning of words and phrases, which enables them to produce coherent and appropriately situated replies.\ To understand the abilities of large language models (LLMs), we compare them with fine-tuned models. LLMs are really big language models that are trained on lots of data without being specifically adjusted for particular tasks. On the other hand, fine-tuned models are usually smaller language models that are also trained and then further adjusted on a smaller dataset that focuses on a specific task. In simple terms, fine-tuned models are more specialized and optimized for specific tasks compared to LLMs.</p>
<p>Now, let’s talk about some practical applications of language models. One important application is natural language understanding. LLMs are really good at understanding and making sense of human language, even when they encounter new or unfamiliar data. This makes them useful for tasks that involve understanding language in different contexts or with very little training data.</p>
<p>Another application is natural language generation. LLMs have the ability to generate text that is coherent, relevant, and of high quality. This can be utilized in various applications where we need computers to create text, such as writing articles, generating chatbot responses, or even creating stories.</p>
<p>Language models are also helpful in knowledge-intensive tasks. LLMs have been trained on vast amounts of data, so they store a lot of knowledge about different domains or general information about the world. This knowledge can be used to assist in tasks that require specific expertise or general understanding.</p>
<p>Lastly, language models can improve reasoning abilities. LLMs are designed to understand patterns and relationships in language, which can be useful for decision-making and problem-solving in different situations. By utilizing the reasoning capabilities of LLMs, we can enhance our ability to make better decisions and solve complex problems.</p>
<p>In the realm of large language models (LLMs), researchers employ different training strategies, model architectures, and use cases. To better understand them, these models can be classified into two main categories: encoder-only language models and decoder-only language models.</p>
<p>Encoder-only language models, also known as Encoder-Decoder models or BERT-style language models, are utilized when there is readily available natural language data. These models are trained using a technique called the Masked Language Model, where the model predicts masked words in a sentence while considering the surrounding context. This training approach allows the model to develop a deeper understanding of the relationships between words and the context in which they are used. These models typically employ the Transformer architecture, a powerful deep learning model that excels at processing and comprehending natural language.</p>
<p>On the other hand, decoder-only language models, such as GPT-style language models, are designed to understand and generate human-like text. These models analyze patterns from a large amount of training data and predict what comes next in a given sequence of words. Unlike encoder-only models, decoder-only models focus on generating text rather than understanding it in a conversational context. They can be employed for tasks like generating creative writing, answering questions, or assisting with language-related tasks. These models are trained as Autoregressive Language Models, where they generate the next word in a sequence based on the preceding words. This type of training, combined with the ability to learn from prompts and in-context information, demonstrates the superiority of autoregressive language models.</p>
<p>Apdditionally, both encoder-only and decoder-only models benefit from few-shot and zero-shot learning. Few-shot learning enables the models to learn new things with just a few examples, while zero-shot learning allows them to learn completely new concepts without any examples at all. These approaches enable the models to perform well on tasks they haven’t been explicitly trained for, leveraging prior knowledge and transferring knowledge from related tasks.</p>


</section>

 ]]></description>
  <category>LLM</category>
  <category>Privacy</category>
  <guid>https://hushh_labs.gitlab.io/gitlab-profile/posts/survey_on_llm_privacy.html</guid>
  <pubDate>Tue, 11 Jul 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Logo Analysis for Brand Affinity</title>
  <dc:creator>Justin Donaldson</dc:creator>
  <link>https://hushh_labs.gitlab.io/gitlab-profile/posts/logo_analysis.html</link>
  <description><![CDATA[ 



<section id="understanding-customer-preferences-with-hushhs-computer-vision-platform" class="level1">
<h1>Understanding Customer Preferences with Hushh’s Computer Vision Platform</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://hushh_labs.gitlab.io/gitlab-profile/posts/images/logos.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Logo Analysis</figcaption>
</figure>
</div>
<p>In today’s digital age, retailers are constantly seeking innovative ways to better understand their customers and build brand affinity. One powerful tool that is revolutionizing the retail industry is Hushh, a customer wallet platform that utilizes machine learning to analyze personal images and identify brand logos. By leveraging the power of computer vision and deep learning algorithms, Hushh provides retailers with valuable insights into customer preferences and behavior.</p>
<section id="how-does-hushh-logo-analysis-work" class="level2">
<h2 class="anchored" data-anchor-id="how-does-hushh-logo-analysis-work">How Does Hushh Logo Analysis Work?</h2>
<p>Hushh’s computer vision platform uses a combination of deep learning and computer vision techniques to identify logos in personal images. Whether it’s a logo on clothing, accessories, or products, Hushh can detect and analyze it. Once the logo is identified, the platform then determines the brand associated with it.</p>
</section>
<section id="capturing-brand-preference-over-time" class="level2">
<h2 class="anchored" data-anchor-id="capturing-brand-preference-over-time">Capturing Brand Preference Over Time</h2>
<p>One of the key features of Hushh is its ability to track customer preferences over time. By analyzing personal images and customer sentiment, the platform provides retailers with valuable insights into how customer preferences evolve. This information is crucial for retailers to tailor their marketing strategies and offerings accordingly.</p>
<p>Brands are a powerful way for users to express their personal preferences publicly. By capturing brand preference over time, retailers can gain a deeper and more immediate insight into what a customer is interested in when they arrive at the store.</p>
<p>Hushh is working on novel information displays to express customer brand preference. The following widget shows a force directed bubble plot of a customer’s favorite brands, sized according to how often that brand appears in their personal images. The widget below is fully interactive, enabling intuitive browsing of brands and brand categories.</p>
<iframe width="780" h<iframe="" height="684" frameborder="0" src="https://observablehq.com/embed/b75d89ea5cb1eccb?cells=chart">
<div id="quarto-navigation-envelope" class="hidden">
<p><span class="hidden" data-render-id="quarto-int-sidebar-title">Hushh Labs</span> <span class="hidden" data-render-id="quarto-int-navbar-title">Hushh Labs</span> <span class="hidden" data-render-id="quarto-int-navbar:Home">Home</span> <span class="hidden" data-render-id="quarto-int-navbar:/index.html">/index.html</span> <span class="hidden" data-render-id="quarto-int-navbar:About">About</span> <span class="hidden" data-render-id="quarto-int-navbar:/about.html">/about.html</span> <span class="hidden" data-render-id="quarto-int-navbar:/index.xml">/index.xml</span></p>
</div>
<div id="quarto-meta-markdown" class="hidden">
<p><span class="hidden" data-render-id="quarto-metatitle">Hushh Labs - Logo Analysis for Brand Affinity</span> <span class="hidden" data-render-id="quarto-twittercardtitle">Hushh Labs - Logo Analysis for Brand Affinity</span> <span class="hidden" data-render-id="quarto-ogcardtitle">Hushh Labs - Logo Analysis for Brand Affinity</span> <span class="hidden" data-render-id="quarto-metasitename">Hushh Labs</span> <span class="hidden" data-render-id="quarto-twittercarddesc">Customers regularly express preferences for brands based on their attire. How can retail adapt and learn without compromising privacy?</span> <span class="hidden" data-render-id="quarto-ogcardddesc">Customers regularly express preferences for brands based on their attire. How can retail adapt and learn without compromising privacy?</span></p>
</div>
</section>
</section>

</main> <!-- /main -->
<script id = "quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->

</body>

</html></iframe></section></section> ]]></description>
  <category>fashion</category>
  <category>ML</category>
  <category>vision</category>
  <guid>https://hushh_labs.gitlab.io/gitlab-profile/posts/logo_analysis.html</guid>
  <pubDate>Tue, 11 Jul 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Searching images using text</title>
  <dc:creator>Justin Donaldson</dc:creator>
  <link>https://hushh_labs.gitlab.io/gitlab-profile/posts/image_search_with_text.html</link>
  <description><![CDATA[ 



<section id="leveraging-llm-embeddings-for-image-search-in-luxury-fashion-a-breakthrough-beyond-tfidf" class="level1">
<h1>Leveraging LLM Embeddings for Image Search in Luxury Fashion: A Breakthrough Beyond TF/IDF!</h1>
<p>In the realm of luxury fashion, image search plays a pivotal role in discovering visually stunning and high-end products. While traditional TF/IDF (Term Frequency-Inverse Document Frequency) search has been a go-to method, the recent emergence of text-generated Large Language Model (LLM) embeddings for image search has revolutionized the way we explore luxury fashion. This cutting-edge approach surpasses the limitations of TF/IDF search, empowering users to find the epitome of elegance and style effortlessly.</p>
<section id="unleashing-the-potential-of-llm-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="unleashing-the-potential-of-llm-embeddings">Unleashing the Potential of LLM Embeddings</h2>
<p>Large Language Models, such as GPT-3.5, possess remarkable natural language processing capabilities. Trained on massive amounts of textual data, these models acquire a deep understanding of language nuances, syntax, and semantics. LLM embeddings are numerical representations derived from text, capturing the intricate contextual information inherent in sentences, paragraphs, or documents.</p>
</section>
<section id="the-superiority-of-llm-embeddings-in-luxury-fashion-image-search" class="level2">
<h2 class="anchored" data-anchor-id="the-superiority-of-llm-embeddings-in-luxury-fashion-image-search">The Superiority of LLM Embeddings in Luxury Fashion Image Search</h2>
<p>By harnessing the power of LLM embeddings, we unlock a myriad of advantages that transform the landscape of luxury fashion image search:</p>
<ol type="1">
<li><p>Enhanced Semantic Relevance LLM embeddings encapsulate a profound comprehension of language semantics, leading to remarkably accurate search results. In the context of luxury fashion, where precision and exquisite details matter, LLM embeddings grasp the subtle nuances and connotations associated with high-end fashion. This semantic awareness ensures that search results align with the desired level of opulence, refinement, and aesthetic appeal.</p></li>
<li><p>Fashion-Specific Vocabulary and Concepts Luxury fashion is an industry replete with distinct terminologies and industry-specific jargon. Traditional TF/IDF search may struggle to capture the essence of these nuanced fashion terms, leading to suboptimal search outcomes. Conversely, LLM embeddings excel in comprehending fashion-specific vocabulary, recognizing brand names, fashion trends, and intricate design elements. This prowess enables users to delve into the world of luxury fashion with unparalleled accuracy.</p></li>
<li><p>Context-Aware Search In the realm of luxury fashion, context plays a pivotal role in understanding the true essence of a product. LLM embeddings excel at capturing the contextual information present in textual descriptions or queries, leading to more refined and contextually appropriate image search results. Users can now effortlessly explore images that encapsulate the desired style, occasion, or fashion statement, ensuring a seamless and personalized luxury fashion journey.</p></li>
<li><p>Uniting Text and Visuals The marriage of text and visuals is at the heart of luxury fashion image search. LLM embeddings act as a bridge, facilitating a seamless connection between the textual descriptions and the visual representation of fashion products. By leveraging LLM embeddings, users can effortlessly navigate the realm of luxury fashion, exploring visually captivating images that align with their refined tastes and preferences.</p></li>
</ol>
<p>In conclusion, the use of LLM embeddings for luxury fashion image search transcends the limitations of conventional TF/IDF search. By harnessing the semantic understanding, fashion-specific vocabulary, and contextual awareness ingrained in LLM embeddings, users can embark on a visually enchanting journey through the world of luxury fashion. Experience the epitome of elegance and style like never before, with LLM-powered image search in luxury fashion.</p>


</section>
</section>

 ]]></description>
  <category>search</category>
  <category>embeddings</category>
  <guid>https://hushh_labs.gitlab.io/gitlab-profile/posts/image_search_with_text.html</guid>
  <pubDate>Mon, 10 Jul 2023 07:00:00 GMT</pubDate>
</item>
</channel>
</rss>
